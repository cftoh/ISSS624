---
title: "Take-home Exercise 1: Geospatial Analytics for Public Good"
date: "21 November 2023"
date-modified:  "last-modified"
format: html
execute:
  echo: true
  eval: true
  warning:  false
editor: visual
---

## Introduction

Digitization of urban infrastructure has introduced new datasets from tracking of movement patterns, including space and time, through GPS and RFID. Understanding these datasets may potentially lead to a more informed decision making process in urban management and planning.

The purpose of this exercise is to perform Exploratory Spatial Data Analysis (ESDA), via appropriate Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

# The TASK

### Geovisualisation and Analysis

-   With reference to the time intervals provided in the table below, compute the passenger trips generated by origin at the hexagon level,

    | Peak hour period             | Station entry time |
    |------------------------------|--------------------|
    | Weekday morning peak         | 6am to 9am         |
    | Weekday afternoon peak       | 5pm to 8pm         |
    | Weekend/holiday morning peak | 11am to 2pm        |
    | Weekend/holiday evening peak | 4pm to 7pm         |

-   Display the geographical distribution of the passenger trips by using appropriate geovisualisation methods,

-   Describe the spatial patterns revealed by the geovisualisation (not more than 200 words per visual).

### Local Indicators of Spatial Association (LISA) Analysis

-   Compute LISA of the passengers trips generate by origin at hexagon level.

-   Display the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e. p-value \< 0.05)

-   With reference to the analysis results, draw statistical conclusions (not more than 200 words per visual).

### Emerging Hot Spot Analysis(EHSA)

With reference to the passenger trips by origin at the hexagon level for the four time intervals given above:

-   Perform Mann-Kendall Test by using the spatio-temporal local Gi\* values,

-   Prepared EHSA maps of the Gi\* values of the passenger trips by origin at the hexagon level. The maps should only display the significant (i.e. p-value \< 0.05).

-   With reference to the EHSA maps and data visualisation prepared, describe the spatial patterns reveled. (not more than 250 words per cluster).

I will be looking to Task 1: GeoVisualisation and Analysis and Task 2: Local Indicators of Spatial Association (LISA) Analysis.

## Setting up

```{r}
#loading in the necessary functions
pacman::p_load(tmap, sf, tidyverse, spdep, sfdep, DT, knitr)
```

# Geospatial Data Wrangling

# Data Import, Extraction and Processing

### Aspatial Data

***Passenger Volume by Origin Destination Bus Stops*** from LTA DataMall

First download the raw data, and import into R environment using read_csv().

```{r}
odbs <- read_csv("data/aspatial/origin_destination_bus_202308.csv")
#choosing Aug data as data set
```

Next we will need to clean up the data sets, first by changing the data type to factor for easy sorting, filter and grouping:

```{r}
odbs$ORIGIN_PT_CODE <- as.factor(odbs$ORIGIN_PT_CODE)
odbs$DESTINATION_PT_CODE <- as.factor(odbs$DESTINATION_PT_CODE)
#This changes the data type from chr to factor in the data field
```

The Data required are:

-   Weekday morning peak (6am to 9am)

-   Weekday afternoon peak (5pm-8pm)

-   Weekend/Holiday morning peak (11am to 2pm)

-   Weekend/Holiday evening peak (4pm to 7pm)

We will next filter our *odbs* to get the required data per above:

```{r}

odbs_peak <- odbs %>% 
  filter((DAY_TYPE == "WEEKDAY" & 
           ((TIME_PER_HOUR >=6 & TIME_PER_HOUR <=9) | 
              (TIME_PER_HOUR >=17 & TIME_PER_HOUR <=20))) | 
           DAY_TYPE == "WEEKENDS/HOLIDAY" & 
           ((TIME_PER_HOUR >=11 & TIME_PER_HOUR <=14) |
              (TIME_PER_HOUR >=16 & TIME_PER_HOUR <=19))) %>%
  group_by(ORIGIN_PT_CODE) %>% #this allow me to extract all those trips generated
  summarize(TRIPS = sum(TOTAL_TRIPS)) #derives new field to allow me to do the aggregation
```

```{r}
#just to take a look at the data table
datatable(odbs_peak)
```

To preserve the data set, we can save the output in rds format for future use:

```{r}
write_rds(odbs_peak, "data/rds/odbs_peak.rds")
```

To import into R environment:

```{r}
odbs_peak <- read_rds("data/rds/odbs_peak.rds")
```

### Geospatial Data

1.  Bus Stop Location from LTA DataMall

```{r}
busstop <- st_read(dsn = "data/geospatial",
                     layer = "BusStop") %>% 
    st_transform(crs = 3414)
  #the st_transform(crs = ?) function takes takes 2D ST_Geometry data as input and returns values converted into the spatial reference specified (new coordinate reference system) provided, in this case 3414 is the Projected coordinate system for SG
```

2.  Hexagon, a [hexagon](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm) layer of 250m (this distance is the perpendicular distance between the centre of the hexagon and its edges.) should be used to replace the relative coarse and irregular Master Plan 2019 Planning Sub-zone GIS data set of URA.

Creating Hexagon

```{r}
busstop_hexagon_grid = st_make_grid(busstop, c(250, 250), what = "polygons", square = FALSE)

hexagon_grid_sf = st_sf(geometry = busstop_hexagon_grid) %>%
  # add grid ID
  mutate(grid_id = 1:length(lengths(busstop_hexagon_grid)))
```

## Data Cleaning and Transformation

We will next use `st_intersection()` for point and polygon overlay, to combine the data sets. This will provide us with output in point sf object.

```{r}
hexagon <- st_intersection(hexagon_grid_sf, busstop) %>%
  select(1,2,4) %>%
  st_drop_geometry()

```

```{r}
write_rds(hexagon, "data/rds/hexagon.rds")
```

Now we have got three data sets, namely

1.  *odbs_peak* : The total number of trips from the Origin Bus Stop during the specified peak period.

2.  *busstop* : The details of bus stop in Singapore, with the location.

3.  *hexagon:* A hexagon layer of 250m with bus stop location.

[Performing the Relational Join (to update attribute table of one geospatial with another aspatial data set)]{.underline}

Now we will next combine this onto our our *odbs_peak* data frame which shows the total number of trips from particular bus stop during peak hour.

```{r}
obs_peak <- left_join(odbs_peak, hexagon,
                      by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE) %>%
  group_by(grid_id) %>% #group by the grid_ID that has values
  summarize(TOTAL_TRIPS = sum(TRIPS)) #derives new field to allow me to do the aggregation of total trips per grid IDs

```

Next, as data sanity, we check if any duplicating records:

```{r}
duplicate <- obs_peak %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

We noted there are duplicating records, so we will need to only retain the unique value using the below:

```{r}
obs_peak <- unique(obs_peak)
```

This removed duplicate records from the data frame, now we have got all unique records .

However, upon closer inspection, noted there are some duplicates on records:

```{r}
datatable(obs_peak)
```

Per the data table above, we can see that grid_id\[4\] and grid id\[128\] share the same TOTAL_TRIP, upon closer check, we noted that the busstop 25059 has the point coordinate that sits between hexagon 4 and hexagon 128.

![](data/busstop_1.png)

```{r}
par(mfrow = c(1, 2))
plot(hexagon_grid_sf[[1]][[4]])
points(3970.122, 28063.28)
plot(hexagon_grid_sf[[1]][[128]])
points(3970.122, 28063.28)

```

We will keep the data sitting on the line of hexagon as it is, for now.

Next, we combine our *obs_peak* data frame onto the *hexagon_grid_sf* data frame to understand the total number of trips per origin bus stop during peak period in hexagon.

```{r}

hexagon_obs_peak <- left_join(hexagon_grid_sf, obs_peak,
            by = "grid_id") %>%
  drop_na("TOTAL_TRIPS","grid_id")  #remove the entries with NA in TRIPS and grid_id




```

```{r}
tmap_mode("view")
tmap_options(check.and.fix = TRUE)
map_honeycomb = tm_shape(hexagon_obs_peak) +
  tm_fill(
    col = "TOTAL_TRIPS",
    palette = "Reds",
    style = "cont",
    title = "Number of Trips",
    alpha = 0.6,
    popup.vars = c("Number of TRIPS: " = "TOTAL_TRIPS"),
    popup.format = list(TOTAL_TRIPS = list(format = "f", digits = 0))) +
  tm_borders(col = "grey40", lwd = 0.7)+
  tm_scale_bar()

map_honeycomb_q = tm_shape(hexagon_obs_peak)+ 
  tm_fill("TOTAL_TRIPS", 
          style = "quantile", 
          palette = "Reds",
          title = "Number of Trips (Quantile)") +
  tm_layout(main.title = "Passenger Trips during Peak Hours",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_borders(alpha = 0.3) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

tmap_arrange(map_honeycomb, map_honeycomb_q, asp=1, ncol=2)



```

Next we take a closer look at those hexagon area that have \>100k total number of trips:

```{r}
tmap_mode("view")
tmap_options(check.and.fix = TRUE)
map_honeycomb = tm_shape(hexagon_obs_peak %>%
                         filter(TOTAL_TRIPS>100000)) +
  tm_fill(
    col = "TOTAL_TRIPS",
    palette = "Reds",
    style = "cont",
    title = "Number of TRIPS",
    id = "grid_id",
    alpha = 0.6,
    popup.vars = c("Number of TRIPS: " = "TOTAL_TRIPS"),
    popup.format = list(TOTAL_TRIPS = list(format = "f", digits = 0))) +
  tm_borders(col = "grey40", lwd = 0.7)


map_honeycomb
```

[Own Analysis]{.underline}

The above map shows that the areas with \>100k total trips during the peak hours are maintain saturated along key residential area, or likely in areas where there is direct bus to cities / CBD (for those commuting to school / work during peak hours).

It is also worth noting that the areas near SG-MY causeway also have high number of total trips during peak period, likely due to people residing in Malaysia heading to Singapore for work / study, likely contributed by weekdays peak period.

# Local Indicators of Spatial Association (LISA) Analysis

Objective is to:

-   Compute LISA of the passengers trips generate by origin at hexagon level.

-   Display the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e. p-value \< 0.05)

-   With reference to the analysis results, draw statistical conclusions (not more than 200 words per visual).

Data Preparation

```{r}

hexagon1 <- read_rds("data/rds/hexagon.rds") 
hexagon1 <- left_join(hexagon1, hexagon_grid_sf,
            by = "grid_id")

#this is to obtain a combined dataframe with bus stop N, grid id and hexagon geometry
```

```{r}
#for simplicity, i will use the total trips taken during the peak period (for all timing, during both weekday and weekends) 

hexagon1_obs_peak <- left_join(hexagon1, odbs_peak,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE")) %>%
  rename(ORIGIN_BS = BUS_STOP_N) %>%
  group_by(grid_id) %>% 
  summarise(TOTAL_TRIPS = sum(TRIPS), ORIGIN_BS = list(ORIGIN_BS))
#just retaining both the total trips and the list of bus stop covered by each hexagon

hexagon1_obs_peak <- left_join(hexagon1_obs_peak, hexagon_grid_sf, by = "grid_id")
```

## Global Measures of Spatial Association

### Step 1: Deriving contiguity spatial weights

```{r}
nb_queen <- hexagon1_obs_peak %>% 
  mutate(nb = st_contiguity(geometry),
         .before = 1)

summary(nb_queen$nb)
```

The summary report above shows that there are 3131 hexagons. The most connected hexagons has 6 neighbours. There are 362 hexagons with only one neighbour.

```{r}
nb_queen

```

### Deriving fixed distance weight

```{r}
hexagon1_obs_peak
```

```{r}

hexagon1_obs_peak <- hexagon1_obs_peak %>%
  drop_na(TOTAL_TRIPS)

hexagon1_peak_sf <- st_as_sf(hexagon1_obs_peak)
geo <- st_geometry(hexagon1_peak_sf)
nb <- st_knn(geo, longlat = TRUE)
dists <- unlist(st_nb_dists(geo, nb))

summary(dists)
```

```{r}
longitude <- map_dbl(hexagon1_obs_peak$geometry, ~st_centroid(.x)[[1]])
latitude <- map_dbl(hexagon1_obs_peak$geometry, ~st_centroid(.x)[[2]])
coords <- cbind(longitude, latitude)

#hexagon1_obs_peak$coords <- coords
#if we need to use centroids

```

```{r}
k1 <- knn2nb(knearneigh(coords, k = 1))
k1dists <- unlist(nbdists(k1, coords))
summary(k1dists)
```

```{r}
hexagon1_obs_peak$ORIGIN_BS[match(max(k1dists), k1dists)]
```

```{r}
wm_d43 <- dnearneigh(coords,0,43000)
wm_d43
```

From the output, we see that the average number of links is 3023.913. The number is quite high and may skew the analysis.

```{r}
str(wm_d43)
```

#### **Adaptive Distance-based Weight Matrix**

To overcome the issue of fixed distance weight matrix where there is uneven distribution of neighbours, we can use directly control the numbers of neighbours using k-nearest neighbours, as shown in the code chunk below.

As a rule-of-thumb, we will set k = 8 i.e., all regions will have 8 neighbours.

```{r}
knn8 <- knn2nb(knearneigh(coords, k=8))
knn8
```

### Which spatial weight matrix to use?

Between contiguity-based and distance-based spatial weight matrices, we lean towards distance-based matrices. Within distance-based matrices, we will select the adaptive distance-based spatial weight matrix for our subsequent analysis, due to:

-   Singapore bus stop are not evenly distributed across the country, some area have more bus stops, more trips. Hence, a contiguity-based matrix will have the issue where hexagon area of certain area may have more neighbours. This would likely skew our analysis. Therefore, distance-based methods are preferred.

-   The fixed distance-based method (with high upper limit) has the disadvantage that some regions would only have zero or 1 neighbour, while on average regions have 3023 neighbours (over 3025 records).

Based on the above, we will select adaptive distance-based spatial weight matrix.

### Row-Standardised Weights Matrix

After selecting the weight matrix to use, we will now assign weights to each neighboring polygon. Each neighboring polygon will be assigned equal weight (style="W") by assigning the fraction 1/(#of neighbors) to each neighbouring area. This is also known as a row-standardised matrix where each row in the matrix sums to 1.

```{r}

rswm_knn8 <- nb2listw(knn8,
                   style = "W",
                   zero.policy = TRUE)
rswm_knn8


```

We will be using the row-standardised weight matrix for the next part of the analysis.

The Moran I statistic ranges from -1 to 1. If the Moran I is:

-   positive (I\>0): Clustered, observations tend to be similar

-   negative (I\<0): Disperse, observations tend to be dissimilar

-   approximately zero: observations arranged randomly over space

```{r}
moran.test(hexagon1_obs_peak$TOTAL_TRIPS,
           listw = rswm_knn8,
           zero.policy = TRUE,
           na.action = na.omit)
```

Since the p-value \< 0.05, we have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone. Since Moran I statistics are larger than 0, the observation are clustered, observations tend to be similar.

Computing Monte Carlo Moran's I

```{r}
set.seed(1234)
bperm_func = moran.mc(hexagon1_obs_peak$TOTAL_TRIPS, 
         listw = rswm_knn8,
         nsim = 999,
         zero.policy = TRUE,
         na.action = na.omit)
bperm_func
```

Since the **pseudo** p-value is \< 0.05, we have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone.

## Computing local Moran's I

```{r}
hexagon1_peak_sf <- hexagon1_peak_sf %>%
  drop_na(TOTAL_TRIPS)

localMI_func <- localmoran(hexagon1_peak_sf$TOTAL_TRIPS, rswm_knn8)
head(localMI_func)
```

*localmoran()* function returns a matrix of values whose columns are:

-   Ii: the local Moran's I statistics

-   E.Ii: the expectation of local moran statistic under the randomisation hypothesis

-   Var.Ii: the variance of local moran statistic under the randomisation hypothesis

-   Z.Ii:the standard deviate of local moran statistic

-   Pr(): the p-value of local moran statistic

### Mapping Local Moran's I values and p-values

```{r}
hpobs.localMI_func <- cbind(hexagon1_peak_sf,localMI_func) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

Visualizing local Moran's I and p-value

```{r}
localMI_func.map <- tm_shape(hpobs.localMI_func) +
  tm_fill(col = "Ii", 
          style = "pretty",
          title = "Local Moran I Statistics") +
  tm_borders(alpha = 0.3) + 
  tm_layout(main.title = "Local Moran's I Map)",
            main.title.size = 1,
            main.title.position = "center",
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE)

pvalue_func.map <- tm_shape(hpobs.localMI_func) + 
                tm_fill(col = "Pr.Ii",
                       breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
                       palette = "-Blues",
                       title = "Local Moran's I p-values") + 
                tm_borders(alpha = 0.3)+ 
  tm_layout(main.title = "Local Moran's I p-values Map)",
            main.title.size = 1,
            main.title.position = "center",
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE)

tmap_arrange(localMI_func.map, pvalue_func.map, asp = 1, ncol = 2)
```

### Visualising LISA map

LISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran's I of geographical areas and their respective p-values.

Data Preparation

```{r}
quadrant <- vector(mode = 'numeric', length = nrow(localMI_func))

hexagon1_peak_sf$lag_pct_func <- lag.listw(rswm_knn8, hexagon1_peak_sf$TOTAL_TRIPS)
DV_func <- hexagon1_peak_sf$lag_pct_func - mean(hexagon1_peak_sf$lag_pct_func)     

LM_I_func <- localMI_func[,1] 

signif <- 0.05

quadrant[DV_func <0 & LM_I_func>0] <- 1 #low-low
quadrant[DV_func >0 & LM_I_func<0] <- 2 #high-low
quadrant[DV_func <0 & LM_I_func<0] <- 3 #low-high
quadrant[DV_func >0 & LM_I_func>0] <- 4 #high-high

quadrant[localMI_func[,5]>signif] <- 0
```

To prepare the LISA cluster map.

```{r}
#Assign each region  to its respective 
hpobs.localMI_func$quadrant <- quadrant
#Set the colours--one for each quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c") 

clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

tm_shape(hpobs.localMI_func) + 
  tm_fill(col = "TOTAL_TRIPS",
          style = "quantile", 
          palette = colors[c(sort(unique(quadrant)))+1],
          labels = clusters[c(sort(unique(quadrant)))+1])  + 
  tm_borders(alpha = 0.3) + 
  tm_layout(main.title = "LISA Cluster Map) ",
            main.title.size = 1,
            main.title.position = "center",
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE)


```

#### Interpretation of Results

At a glance, in the LISA map, we can see the larger clusters situation near the top, left and right of map. These areas are the key residential areas where many commuter may be taking bus to school / work during the peak hours. . We also notice no significant spatial clusters in the middle/bottom area of the map, these are less residential area and are mostly town area where people are heading into (destination), instead of leaving from (origin), during the peak hours..

# Additional

Note: This is purely for my own curiosity.

```{r}
mpsz <- st_read(dsn="data/geospatial",layer="MPSZ-2019") %>% 
  st_transform(crs = 3414)
```

```{r}
busstop_mpsz <- st_intersection(busstop, mpsz) %>%
  select(BUS_STOP_N, SUBZONE_C) %>%
  st_drop_geometry()
```

```{r}
obs_peak <- left_join(odbs_peak, busstop_mpsz,
                      by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_SZ = SUBZONE_C)

obs_peak <- unique(obs_peak)
```

```{r}
mpsz_obs_peak <- left_join(mpsz, 
                           obs_peak,
                           by = c("SUBZONE_C" = "ORIGIN_SZ"))
```

Preparing a choropleth map showing the distribution of passenger trips at planning sub-zone level:

```{r}
mpsz_obs_peak %>%
  drop_na(TRIPS)
tm_shape(mpsz_obs_peak)+ 
  tm_fill("TRIPS", 
          style = "quantile", 
          palette = "Reds",
          title = "Passenger trips") +
  tm_layout(main.title = "Passenger Trips during Peak Hours",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_borders(alpha = 0.3) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2) +
  tm_credits("Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\n and Population data from Department of Statistics DOS", 
             position = c("left", "bottom"))
```

An extra step to look at the distribution of the total number of trips during peak period broken down by Region distribution.

```{r}
mpsz_obs_peak %>%
  drop_na(TRIPS) #this is remove NA data

agg_mpsz_obs_peak_byzones <- mpsz_obs_peak %>%
  filter(TRIPS != "NA") %>%
  group_by(REGION_N) %>%
  summarize(TRIPS = sum(TRIPS)) %>%
  st_drop_geometry()
```

```{r}
library(scales)
ggplot(agg_mpsz_obs_peak_byzones) +
  geom_bar(aes(x=REGION_N, 
               y=TRIPS,
               fill=REGION_N),
           stat="identity") +
  scale_y_continuous(labels = label_number(suffix = "M", scale = 1e-6)) +
  labs(title="Number of TRIPS during Peak Period",
       subtitle="(sorted by Region)",
       y="Number of Trips",
       x="Region")+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.line.y=element_line(colour="grey50"),
        legend.text = element_text(size = 8))
```

Although we have noted from previous data analysis that the highest number of trip originated from West Region, the above bar chart shows that in total the number of trips in central region is still the highest. Breaking it down further, we can see from the count below that there is significantly larger number of bus stop stations in Central Area, which likely to be the factor attributing to the highest aggregate number of trips across the regions. Another deduction is that the commuters taking trips in central area may take shorter bus ride, or make more transfers within the central region than those taking the bus stop other regions.

```{r}
table(mpsz_obs_peak$REGION_N)
```

How about if we do the analysis based on subzone?

```{r}
mpsz_obs_peak %>%
  drop_na(TRIPS) #this is remove NA data


agg_mpsz_obs_peak_bysubzones <- mpsz_obs_peak %>%
  filter(TRIPS != "NA") %>%
  group_by(SUBZONE_N) %>%
  summarize(TRIPS = sum(TRIPS)) %>%
  st_drop_geometry()
```

We can see there is a total of 313 subzones, once aggregated.

```{r}
library(scales)
ggplot(agg_mpsz_obs_peak_bysubzones) +
  geom_bar(aes(x=SUBZONE_N, 
               y=TRIPS),
           stat="identity") +
  scale_y_continuous(labels = label_number(suffix = "M", scale = 1e-6)) +
  labs(title="Number of TRIPS during Peak Period",
       subtitle="(sorted by Region)",
       y="Number of Trips",
       x="Region")+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.line.y=element_line(colour="grey50"),
        legend.text = element_text(size = 8))
```

```{r}
top_n(agg_mpsz_obs_peak_bysubzones, n=10, TRIPS) %>%
          ggplot(., aes(x=SUBZONE_N, 
                        y=TRIPS,
                        fill=SUBZONE_N))+
              geom_bar(stat='identity') +
  scale_y_continuous(labels = label_number(suffix = "M", scale = 1e-6)) +
  labs(title="Top 10 Highest number of TRIPS during Peak Period",
       subtitle="(sorted by SubZones)",
       y="Number of Trips",
       x="SubZones")+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.line.y=element_line(colour="grey50"),
        legend.text = element_text(size = 8))

```

# 
