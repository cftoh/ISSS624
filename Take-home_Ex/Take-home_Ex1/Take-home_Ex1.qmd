---
title: "Take-home Exercise 1: Geospatial Analytics for Public Good"
date: "21 November 2023"
date-modified:  "last-modified"
format: html
execute:
  echo: true
  eval: true
  warning:  false
editor: visual
---

## Introduction

Digitization of urban infrastructure has introduced new datasets from tracking of movement patterns, including space and time, through GPS and RFID. Understanding these datasets may potentially lead to a more informed decision making process in urban management and planning.

The purpose of this exercise is to perform Exploratory Spatial Data Analysis (ESDA), via appropriate Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

# The TASK

### Geovisualisation and Analysis

-   With reference to the time intervals provided in the table below, compute the passenger trips generated by origin at the hexagon level,

    | Peak hour period             | Station entry time |
    |------------------------------|--------------------|
    | Weekday morning peak         | 6am to 9am         |
    | Weekday afternoon peak       | 5pm to 8pm         |
    | Weekend/holiday morning peak | 11am to 2pm        |
    | Weekend/holiday evening peak | 4pm to 7pm         |

-   Display the geographical distribution of the passenger trips by using appropriate geovisualisation methods,

-   Describe the spatial patterns revealed by the geovisualisation (not more than 200 words per visual).

### Local Indicators of Spatial Association (LISA) Analysis

-   Compute LISA of the passengers trips generate by origin at hexagon level.

-   Display the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e. p-value \< 0.05)

-   With reference to the analysis results, draw statistical conclusions (not more than 200 words per visual).

### Emerging Hot Spot Analysis(EHSA)

With reference to the passenger trips by origin at the hexagon level for the four time intervals given above:

-   Perform Mann-Kendall Test by using the spatio-temporal local Gi\* values,

-   Prepared EHSA maps of the Gi\* values of the passenger trips by origin at the hexagon level. The maps should only display the significant (i.e. p-value \< 0.05).

-   With reference to the EHSA maps and data visualisation prepared, describe the spatial patterns reveled. (not more than 250 words per cluster).

## Setting up

```{r}
#loading in the necessary functions
pacman::p_load(tmap, sf, tidyverse, spdep)
```

# Geospatial Data Wrangling

# Data Import, Extraction and Processing

### Aspatial Data

***Passenger Volume by Origin Destination Bus Stops*** from LTA DataMall

First download the raw data, and import into R environment using read_csv().

```{r}
odbs <- read_csv("data/aspatial/origin_destination_bus_202308.csv")
#choosing Aug data as data set
```

Next we will need to clean up the data sets, first by changing the data type to factor for easy sorting, filter and grouping:

```{r}
odbs$ORIGIN_PT_CODE <- as.factor(odbs$ORIGIN_PT_CODE)
odbs$DESTINATION_PT_CODE <- as.factor(odbs$DESTINATION_PT_CODE)
#This changes the data type from chr to factor in the data field
```

The Data required are:

-   Weekday morning peak (6am to 9am)

-   Weekday afternoon peak (5pm-8pm)

-   Weekend/Holiday morning peak (11am to 2pm)

-   Weekend/Holiday evening peak (4pm to 7pm)

We will next filter our *odbs* to get the required data per above:

```{r}

odbs_peak <- odbs %>% 
  filter((DAY_TYPE == "WEEKDAY" & 
           ((TIME_PER_HOUR >=6 & TIME_PER_HOUR <=9) | 
              (TIME_PER_HOUR >=17 & TIME_PER_HOUR <=20))) | 
           DAY_TYPE == "WEEKENDS/HOLIDAY" & 
           ((TIME_PER_HOUR >=11 & TIME_PER_HOUR <=14) |
              (TIME_PER_HOUR >=16 & TIME_PER_HOUR <=19))) %>%
  group_by(ORIGIN_PT_CODE) %>% #this allow me to extract all those trips generated
  summarize(TRIPS = sum(TOTAL_TRIPS)) #derives new field to allow me to do the aggregation
```

To preserve the data set, we can save the output in rds format for future use:

```{r}
write_rds(odbs_peak, "data/rds/odbs_peak.rds")
```

To import into R environment:

```{r}
odbs_peak <- read_rds("data/rds/odbs_peak.rds")
```

### Geospatial Data

1.  Bus Stop Location from LTA DataMall

```{r}
busstop <- st_read(dsn = "data/geospatial",
                     layer = "BusStop") %>% 
    st_transform(crs = 3414)
  #the st_transform(crs = ?) function takes takes 2D ST_Geometry data as input and returns values converted into the spatial reference specified (new coordinate reference system) provided, in this case 3414 is the Projected coordinate system for SG
```

```{r}
glimpse(busstop)
```

2.  Hexagon, a [hexagon](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm) layer of 250m is provided to replace the relative coarse and irregular Master Plan 2019 Planning Sub-zone GIS data set of URA.

```{r}
mpsz <- st_read(dsn="data/geospatial",layer="MPSZ-2019") %>% 
  st_transform(crs = 3414)
```

```{r}
glimpse(mpsz)
```

## Data Cleaning and Transformation

Now we have got three data sets, namely

1.  *odbs_peak* : The total number of trips from the Origin Bus Stop during the specified peak period.
2.  *busstop* : The details of bus stop in Singapore, with the location.
3.  *mpsz* : The sub-zone boundary of URA Master Plan 2019.

We will next use `st_intersection()` for point and polygon overlay, to combine the *busstop* and *mpsz* data sets. This will provide us with output in point sf object.

Next, `select()` of dplyr package is uses to retain only BUS_STOP_N and SUBZONE_C in the *busstop_mpsz* sf data frame.

```{r}
busstop_mpsz <- st_intersection(busstop, mpsz) %>%
  select(BUS_STOP_N, SUBZONE_C) %>%
  st_drop_geometry() #this removes the geometry (points) field
```

```{r}
glimpse(busstop_mpsz)
```

It is noted that *busstop_mpsz* has 5156 entries compared to *busstop* which has 5161 entries. This indicates that there are 5 bus stops that are excluded in the resultant data frame because they are outside of Singapore boundary (per *mpsz*).

Before moving to the next step, we should SAVE OUTPUT INTO RDS FORMAT :

```{r}
write_rds(busstop_mpsz, "data/rds/busstop_mpsz.csv") 
```

Now we have got the *busstop_mpsz* data frame that shows the bus stops and their locations (in term of subzone), we will next combine this onto our our *odbs_peak* data frame which shows the total number of trips from particular bus stop during peak hour.

```{r}
obs_peak <- left_join(odbs_peak, busstop_mpsz,
                      by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_SZ = SUBZONE_C)
```

Next, as data sanity, we check if any duplicating records:

```{r}
duplicate <- obs_peak %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

We noted there is a total of 26 duplicating records, so we will need to only retain the unique value using the below:

```{r}
obs_peak <- unique(obs_peak)
```

This removed 13 (duplicate) records from the data frame, now we have got 5070 records that are all unique.

Next, we combine our *obs_peak* data frame onto the *mpsz* data frame to understand the total number of trips per origin bus stop during peak period in each planning sub zone per the Master Plan 2019 Planning Sub-zone GIS data set of URA.

```{r}
mpsz_obs_peak <- left_join(mpsz, 
                           obs_peak,
                           by = c("SUBZONE_C" = "ORIGIN_SZ"))
```

# Geospatial Analysis

Assessment point: to use the appropriate thematic and analytics mapping techniques and R functions introduced in class to analysis the geospatial data prepared. You will be assessed on your ability to derive analytical maps by using appropriate rate mapping techniques.

## Choropleth Visualisation

Preparing a choropleth map showing the distribution of passenger trips at planning sub-zone level:

```{r}
mpsz_obs_peak %>%
  drop_na(TRIPS) #this is remove NA data, as it is noted that there are a number of NA data in TRIP column 

tm_shape(mpsz_obs_peak)+ 
  tm_fill("TRIPS", 
          style = "quantile", 
          palette = "Reds",
          title = "Passenger trips") +
  tm_layout(main.title = "Passenger Trips during Peak Hours",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_borders(alpha = 0.3) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2) +
  tm_credits("Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\n and Population data from Department of Statistics DOS", 
             position = c("left", "bottom"))
```

From the first glance of the geovisualization, we can observe that there are a few highly dense area which indicates high number of trips taken from those areas during peak period. The areas seem to be in the popular residential areas.

An extra step to look at the distribution of the total number of trips during peak period broken down by Region distribution.

```{r}
mpsz_obs_peak %>%
  drop_na(TRIPS) #this is remove NA data

agg_mpsz_obs_peak_byzones <- mpsz_obs_peak %>%
  filter(TRIPS != "NA") %>%
  group_by(REGION_N) %>%
  summarize(TRIPS = sum(TRIPS)) %>%
  st_drop_geometry()
```

```{r}
library(scales)
ggplot(agg_mpsz_obs_peak_byzones) +
  geom_bar(aes(x=REGION_N, 
               y=TRIPS,
               fill=REGION_N),
           stat="identity") +
  scale_y_continuous(labels = label_number(suffix = "M", scale = 1e-6)) +
  labs(title="Number of TRIPS during Peak Period",
       subtitle="(sorted by Region)",
       y="Number of Trips",
       x="Region")+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.line.y=element_line(colour="grey50"),
        legend.text = element_text(size = 8))
```

Although we have noted from previous data analysis that the highest number of trip originated from West Region, the above bar chart shows that in total the number of trips in central region is still the highest. Breaking it down further, we can see from the count below that there is significantly larger number of bus stop stations in Central Area, which likely to be the factor attributing to the highest aggregate number of trips across the regions. Another deduction is that the commuters taking trips in central area may take shorter bus ride, or make more transfers within the central region than those taking the bus stop other regions.

```{r}
table(mpsz_obs_peak$REGION_N)
```

How about if we do the analysis based on subzone?

```{r}
mpsz_obs_peak %>%
  drop_na(TRIPS) #this is remove NA data


agg_mpsz_obs_peak_bysubzones <- mpsz_obs_peak %>%
  filter(TRIPS != "NA") %>%
  group_by(SUBZONE_N) %>%
  summarize(TRIPS = sum(TRIPS)) %>%
  st_drop_geometry()
```

We can see there is a total of 313 subzones, once aggregated.

```{r}
library(scales)
ggplot(agg_mpsz_obs_peak_bysubzones) +
  geom_bar(aes(x=SUBZONE_N, 
               y=TRIPS),
           stat="identity") +
  scale_y_continuous(labels = label_number(suffix = "M", scale = 1e-6)) +
  labs(title="Number of TRIPS during Peak Period",
       subtitle="(sorted by Region)",
       y="Number of Trips",
       x="Region")+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.line.y=element_line(colour="grey50"),
        legend.text = element_text(size = 8))
```

```{r}
top_n(agg_mpsz_obs_peak_bysubzones, n=10, TRIPS) %>%
          ggplot(., aes(x=SUBZONE_N, 
                        y=TRIPS,
                        fill=SUBZONE_N))+
              geom_bar(stat='identity') +
  scale_y_continuous(labels = label_number(suffix = "M", scale = 1e-6)) +
  labs(title="Top 10 Highest number of TRIPS during Peak Period",
       subtitle="(sorted by SubZones)",
       y="Number of Trips",
       x="SubZones")+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.line.y=element_line(colour="grey50"),
        legend.text = element_text(size = 8))

```

# Geovisualisation and Geocommunication

Assessment point: you will be assessed on your ability to communicate the complex spatial statistics results in business friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis

### Computing Contiguity Spatial Weight

Before we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area.

To compute Queen contiguity weight matrix:

```{r}
mpsz_obs_peak <- mpsz_obs_peak %>%
  drop_na(TRIPS) #this is remove NA data


wm_q <- poly2nb(mpsz_obs_peak, 
                queen=TRUE)
summary(wm_q)
```

### Row-standardised weights matrix

To prepare for the next analysis, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style="W"). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values.

```{r}
rswm_q <- nb2listw(wm_q, 
                   style="W", 
                   zero.policy = TRUE)
rswm_q
```

### Local Indicators of Spatial Association (LISA) Analysis

-   Compute LISA of the passengers trips generate by origin at hexagon level.

-   Display the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e. p-value \< 0.05)

-   With reference to the analysis results, draw statistical conclusions (not more than 200 words per visual).

LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. In this case we are studying the number of trips rates among various regions in Singapore in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.

We will apply appropriate LISA, especially local Moran'I to detect cluster and/or outlier from number of trips during peak period in various subzones in Singapore.

### Computing Local Moran's I

To compute local Moran's I, the [*localmoran()*](https://r-spatial.github.io/spdep/reference/localmoran.html) function of **spdep** will be used. It computes *Ii* values, given a set of *zi* values and a *listw* object providing neighbour weighting information for the polygon associated with the *zi* values.

The code chunks below are used to compute local Moran's I of *Number of Trips during peak hour* at the subzone level.

```{r}

fips <- order(mpsz_obs_peak$SUBZONE_N)
localMI <- localmoran(mpsz_obs_peak$TRIPS, rswm_q)
head(localMI)
```

Now we have the the localMI, to create the local Moran's I map, we will first append the local Moran's I dataframe (i.e. localMI) onto the mpsz_obs_peak DataFrame:

```{r}
mpsz_obs_peak.localMI <- cbind(mpsz_obs_peak,localMI) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

#### Mapping local Moran's I values

To plot the local Moran's I values:

```{r}
tm_shape(mpsz_obs_peak.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty",
          palette = "RdBu",
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)

```

#### Mapping local Moran's I p-values

The choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.

The code chunks below produce a choropleth map of Moran's I p-values by using functions of **tmap** package.

```{r}
tm_shape(mpsz_obs_peak.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)
```

#### Mapping both local Moran's I values and p-values

```{r}
localMI.map <- tm_shape(mpsz_obs_peak.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)

pvalue.map <- tm_shape(mpsz_obs_peak.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)
```

#### Computing Monte Carlo Moran's I

```{r}
set.seed(1234)
bperm= moran.mc(mpsz_obs_peak$TRIPS, 
                listw=rswm_q, 
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
bperm
```

#### Visualising Monte Carlo Moran's I

```{r}
mean(bperm$res[1:999])
var(bperm$res[1:999])
summary(bperm$res[1:999])

hist(bperm$res, 
     freq=TRUE, 
     breaks=20, 
     xlab="Simulated Moran's I")
abline(v=0, 
       col="red") 
```

### Emerging Hot Spot Analysis(EHSA)
